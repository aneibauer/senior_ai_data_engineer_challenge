"""
Spark batch processing module for streamed commerce events.

This module processes parquet files generated by the Spark streaming application and loads
them into PostgreSQL for analytics. It monitors a designated directory for new parquet
files, processes them in batches using Spark, and archives the processed files.

Key Features:
- Automatic discovery of new parquet files with age-based filtering
- Batch processing using Spark for efficient data handling
- PostgreSQL integration with append mode for incremental loading
- File archiving to prevent reprocessing

Future Enhancements:
- Add scheduler (e.g., Apache Airflow, cron, etc) to run batch jobs every few minutes
- Implement error handling and retry mechanisms for failed batches
- Add data quality checks and validation before loading to PostgreSQL
- Monitoring and alerting for batch job failures
- Support for multiple output formats and destinations
  * schema evolution can be addressed by using the `mergeSchema` option in Spark
  * output table partitioning can be implemented for better performance
- Configuration management for database credentials and paths
"""

import os
import shutil
from pyspark.sql import SparkSession
import time
from typing import List

PARQUET_DIR = "/opt/spark/parquet_output"
ARCHIVE_DIR = os.path.join(PARQUET_DIR, "archive")

POSTGRES_URL = "jdbc:postgresql://postgres:5432/events"
POSTGRES_PROPERTIES = { #TODO: don't hardcode these here!
    "user": "user",
    "password": "password",
    "driver": "org.postgresql.Driver"
}

def get_parquet_files(directory, min_age_secs=30) -> List[str]:
    """
    Returns a list of Parquet file paths from the specified directory that are older than a given minimum age.

    Args:
        directory (str): The path to the directory to search for Parquet files.
        min_age_secs (int, optional): The minimum age in seconds a file must be to be included in the result. Defaults to 30.

    Returns:
        List[str]: A list of file paths to Parquet files that are older than the specified minimum age.

    Notes:
        - Only files with a ".parquet" extension are considered.
        - Files must be regular files (not directories).
        - The age check helps avoid processing files that may still be in the process of being written.
    """
    now = time.time()
    return [
        os.path.join(directory, f)
        for f in os.listdir(directory)
        if (
            f.endswith(".parquet") and 
            os.path.isfile(os.path.join(directory, f)) and
            (now - os.path.getmtime(os.path.join(directory, f))) > min_age_secs #helps avoid partial writes
        )
    ]

def archive_files(files, archive_dir=ARCHIVE_DIR) -> None:
    """
    Moves the specified files to an archive directory.
    This function creates the archive directory if it does not exist, then moves each file in the `files` list to the archive directory, preserving the original file names.
    
    Args:
        files (list): List of file paths to be archived.
        archive_dir (str, optional): Path to the archive directory. Defaults to ARCHIVE_DIR.

    Returns:
        None
    """
    os.makedirs(archive_dir, exist_ok=True)
    for f in files:
        shutil.move(f, os.path.join(archive_dir, os.path.basename(f)))


def main() -> None:
    # Initialize Spark session
    # Scaling up would require a cluster setup, so for now we use local mode
    spark = SparkSession.builder \
        .appName("BatchParquetToPostgres") \
        .master("local[*]") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    #TODO: add more sophisticated logging
    print(f"Looking for parquet files in {PARQUET_DIR}...")
    parquet_files = get_parquet_files(PARQUET_DIR)
    print(parquet_files)
    print(f"Found {len(parquet_files)} parquet files to process.")
    if not parquet_files:
        print("No new parquet files to process.")
        return

    print(f"Processing files: {parquet_files}")

    # Read all new parquet files into a single DataFrame
    df = spark.read.parquet(*parquet_files)

    # Write to Postgres via jdbc connection (append mode)
    df.write.jdbc(
        url=POSTGRES_URL,
        table="commerce_events_table",  # creates table from dataframe schema if it doesn't exist
        mode="append",
        properties=POSTGRES_PROPERTIES
    )

    # Archive processed files
    archive_files(parquet_files)
    print(f"Archived files: {parquet_files}")

    spark.stop()

if __name__ == "__main__":
    
    main()